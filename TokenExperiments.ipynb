{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "dataset_files = list(Path(\"Datasets/jsfakes-tok/bpe/\").glob(\"**/*.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "the JSON object must be str, bytes or bytearray, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Misc Coding Projects\\MusicAI\\MusicGen\\BachGPT.ipynb Cell 2\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Misc%20Coding%20Projects/MusicAI/MusicGen/BachGPT.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m (file,\u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Misc%20Coding%20Projects/MusicAI/MusicGen/BachGPT.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         f_dict \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(f)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Misc%20Coding%20Projects/MusicAI/MusicGen/BachGPT.ipynb#W1sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         res \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mloads(f_dict[\u001b[39m'\u001b[39;49m\u001b[39mids\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Misc%20Coding%20Projects/MusicAI/MusicGen/BachGPT.ipynb#W1sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         idList\u001b[39m.\u001b[39mappend(res)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Misc%20Coding%20Projects/MusicAI/MusicGen/BachGPT.ipynb#W1sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(columns\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[1;32mD:\\Programms\\Anaconda\\Lib\\json\\__init__.py:339\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    338\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(s, (\u001b[39mbytes\u001b[39m, \u001b[39mbytearray\u001b[39m)):\n\u001b[1;32m--> 339\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mthe JSON object must be str, bytes or bytearray, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    340\u001b[0m                         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnot \u001b[39m\u001b[39m{\u001b[39;00ms\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n",
      "\u001b[1;31mTypeError\u001b[0m: the JSON object must be str, bytes or bytearray, not list"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "idList = []\n",
    "for file in dataset_files:\n",
    "    with open (file,\"rb\") as f:\n",
    "        f_dict = json.load(f)\n",
    "        idList.append(list(f_dict['ids']))\n",
    "df = pd.DataFrame(columns=[\"train\"])\n",
    "df[\"train\"]=idList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Datasets/jsfakes-bpe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"csv\", data_files=\"Datasets/jsfakes-bpe.csv\")\n",
    "\n",
    "# Convert dataset to dataframe\n",
    "ds = dataset[\"train\"]\n",
    "df = ds.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': 1889,\n",
       " 'train': '[1700, 9721, 9835, 935, 4149, 4528, 1341, 5269, 577, 43, 2648, 2952, 3477, 1304, 2894, 38, 3253, 1585, 570, 3253, 577, 38, 3666, 391, 732, 3691, 570, 3028, 655, 40, 1341, 378, 8936, 710, 43, 742, 3573, 45, 742, 2789, 46, 221, 5154, 739, 1116, 1517, 3086, 4266, 262, 688, 3464, 2833, 3269, 9324, 1744, 55, 739, 3253, 2266, 1703, 378, 287, 1388, 1287, 41, 508, 739, 254, 629, 64, 516, 1899, 1198, 1675, 242, 2003, 50, 242, 789, 44, 311, 6012, 47, 4533, 664, 47, 2648, 391, 577, 50, 1157, 1509, 2963, 2047, 1084, 3922, 1671, 3984, 557, 1675, 1084, 2968, 1494, 4219, 1084, 5320, 8348, 1478, 33, 6962, 3795, 4453, 6229, 1606, 59, 159, 9850, 3984, 9725]'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take some sample from the dataset\n",
    "raw_datasets = ds.train_test_split(test_size=0.1, shuffle=True)\n",
    "sample= raw_datasets[\"train\"][0]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[1700, 9721, 9835, 935, 4149, 4528, 1341, 5269, 577, 43, 2648, 2952, 3477, 1304, 2894, 38, 3253, 1585, 570, 3253, 577, 38, 3666, 391, 732, 3691, 570, 3028, 655, 40, 1341, 378, 8936, 710, 43, 742, 3573, 45, 742, 2789, 46, 221, 5154, 739, 1116, 1517, 3086, 4266, 262, 688, 3464, 2833, 3269, 9324, 1744, 55, 739, 3253, 2266, 1703, 378, 287, 1388, 1287, 41, 508, 739, 254, 629, 64, 516, 1899, 1198, 1675, 242, 2003, 50, 242, 789, 44, 311, 6012, 47, 4533, 664, 47, 2648, 391, 577, 50, 1157, 1509, 2963, 2047, 1084, 3922, 1671, 3984, 557, 1675, 1084, 2968, 1494, 4219, 1084, 5320, 8348, 1478, 33, 6962, 3795, 4453, 6229, 1606, 59, 159, 9850, 3984, 9725]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[1700,', (0, 6)),\n",
       " ('9721,', (7, 12)),\n",
       " ('9835,', (13, 18)),\n",
       " ('935,', (19, 23)),\n",
       " ('4149,', (24, 29)),\n",
       " ('4528,', (30, 35)),\n",
       " ('1341,', (36, 41)),\n",
       " ('5269,', (42, 47)),\n",
       " ('577,', (48, 52)),\n",
       " ('43,', (53, 56)),\n",
       " ('2648,', (57, 62)),\n",
       " ('2952,', (63, 68)),\n",
       " ('3477,', (69, 74)),\n",
       " ('1304,', (75, 80)),\n",
       " ('2894,', (81, 86)),\n",
       " ('38,', (87, 90)),\n",
       " ('3253,', (91, 96)),\n",
       " ('1585,', (97, 102)),\n",
       " ('570,', (103, 107)),\n",
       " ('3253,', (108, 113)),\n",
       " ('577,', (114, 118)),\n",
       " ('38,', (119, 122)),\n",
       " ('3666,', (123, 128)),\n",
       " ('391,', (129, 133)),\n",
       " ('732,', (134, 138)),\n",
       " ('3691,', (139, 144)),\n",
       " ('570,', (145, 149)),\n",
       " ('3028,', (150, 155)),\n",
       " ('655,', (156, 160)),\n",
       " ('40,', (161, 164)),\n",
       " ('1341,', (165, 170)),\n",
       " ('378,', (171, 175)),\n",
       " ('8936,', (176, 181)),\n",
       " ('710,', (182, 186)),\n",
       " ('43,', (187, 190)),\n",
       " ('742,', (191, 195)),\n",
       " ('3573,', (196, 201)),\n",
       " ('45,', (202, 205)),\n",
       " ('742,', (206, 210)),\n",
       " ('2789,', (211, 216)),\n",
       " ('46,', (217, 220)),\n",
       " ('221,', (221, 225)),\n",
       " ('5154,', (226, 231)),\n",
       " ('739,', (232, 236)),\n",
       " ('1116,', (237, 242)),\n",
       " ('1517,', (243, 248)),\n",
       " ('3086,', (249, 254)),\n",
       " ('4266,', (255, 260)),\n",
       " ('262,', (261, 265)),\n",
       " ('688,', (266, 270)),\n",
       " ('3464,', (271, 276)),\n",
       " ('2833,', (277, 282)),\n",
       " ('3269,', (283, 288)),\n",
       " ('9324,', (289, 294)),\n",
       " ('1744,', (295, 300)),\n",
       " ('55,', (301, 304)),\n",
       " ('739,', (305, 309)),\n",
       " ('3253,', (310, 315)),\n",
       " ('2266,', (316, 321)),\n",
       " ('1703,', (322, 327)),\n",
       " ('378,', (328, 332)),\n",
       " ('287,', (333, 337)),\n",
       " ('1388,', (338, 343)),\n",
       " ('1287,', (344, 349)),\n",
       " ('41,', (350, 353)),\n",
       " ('508,', (354, 358)),\n",
       " ('739,', (359, 363)),\n",
       " ('254,', (364, 368)),\n",
       " ('629,', (369, 373)),\n",
       " ('64,', (374, 377)),\n",
       " ('516,', (378, 382)),\n",
       " ('1899,', (383, 388)),\n",
       " ('1198,', (389, 394)),\n",
       " ('1675,', (395, 400)),\n",
       " ('242,', (401, 405)),\n",
       " ('2003,', (406, 411)),\n",
       " ('50,', (412, 415)),\n",
       " ('242,', (416, 420)),\n",
       " ('789,', (421, 425)),\n",
       " ('44,', (426, 429)),\n",
       " ('311,', (430, 434)),\n",
       " ('6012,', (435, 440)),\n",
       " ('47,', (441, 444)),\n",
       " ('4533,', (445, 450)),\n",
       " ('664,', (451, 455)),\n",
       " ('47,', (456, 459)),\n",
       " ('2648,', (460, 465)),\n",
       " ('391,', (466, 470)),\n",
       " ('577,', (471, 475)),\n",
       " ('50,', (476, 479)),\n",
       " ('1157,', (480, 485)),\n",
       " ('1509,', (486, 491)),\n",
       " ('2963,', (492, 497)),\n",
       " ('2047,', (498, 503)),\n",
       " ('1084,', (504, 509)),\n",
       " ('3922,', (510, 515)),\n",
       " ('1671,', (516, 521)),\n",
       " ('3984,', (522, 527)),\n",
       " ('557,', (528, 532)),\n",
       " ('1675,', (533, 538)),\n",
       " ('1084,', (539, 544)),\n",
       " ('2968,', (545, 550)),\n",
       " ('1494,', (551, 556)),\n",
       " ('4219,', (557, 562)),\n",
       " ('1084,', (563, 568)),\n",
       " ('5320,', (569, 574)),\n",
       " ('8348,', (575, 580)),\n",
       " ('1478,', (581, 586)),\n",
       " ('33,', (587, 590)),\n",
       " ('6962,', (591, 596)),\n",
       " ('3795,', (597, 602)),\n",
       " ('4453,', (603, 608)),\n",
       " ('6229,', (609, 614)),\n",
       " ('1606,', (615, 620)),\n",
       " ('59,', (621, 624)),\n",
       " ('159,', (625, 629)),\n",
       " ('9850,', (630, 635)),\n",
       " ('3984,', (636, 641)),\n",
       " ('9725]', (642, 647))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "\n",
    "# We need to specify the UNK token\n",
    "new_tokenizer = Tokenizer(model=WordLevel(unk_token=\"[UNK]\"))\n",
    "\n",
    "# Add pretokenizer\n",
    "from tokenizers.pre_tokenizers import WhitespaceSplit\n",
    "\n",
    "new_tokenizer.pre_tokenizer = WhitespaceSplit()\n",
    "\n",
    "# Let's test our pre_tokenizer\n",
    "new_tokenizer.pre_tokenizer.pre_tokenize_str(sample[\"train\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yield batches of 1,000 texts\n",
    "def get_training_corpus():\n",
    "  dataset = raw_datasets[\"train\"]\n",
    "  for i in range(0, len(dataset), 1000):\n",
    "    yield dataset[i : i + 1000][\"text\"]\n",
    "\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "\n",
    "# Add special tokens\n",
    "trainer = WordLevelTrainer(\n",
    "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    ")\n",
    "\n",
    "# Post-processing\n",
    "new_tokenizer.save(\"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
